{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import & Install Library"
      ],
      "metadata": {
        "id": "qf0D9zcl-lq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "!pip install gensim scikit-learn nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUd--MLNvXcZ",
        "outputId": "a0a36c45-a680-424f-81d9-a2b94d5993a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import json\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7X2hq6_4-q6P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('myim3_dataset.csv')\n",
        "df = df.drop(columns=['appVersion'], errors='ignore')\n",
        "print(f\"\\nJumlah data awal: {len(df)}\")\n",
        "print(\"\\nDistribusi score awal:\")\n",
        "print(df['score'].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08_Djaes-2jv",
        "outputId": "0351bfe4-4396-416f-e913-e9b3ab146ac9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jumlah data awal: 10000\n",
            "\n",
            "Distribusi score awal:\n",
            "score\n",
            "1    4469\n",
            "2     654\n",
            "3     611\n",
            "4     639\n",
            "5    3627\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "rl6oh-bG-u8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi score ke label sentimen\n",
        "# Score 1-2 = Negatif (0), Score 4-5 = Positif (1), Score 3 = Netral (hapus)\n",
        "def score_to_label(score):\n",
        "    if score <= 3:\n",
        "        return 'negatif'\n",
        "    elif score >= 4:\n",
        "        return 'positif'\n",
        "    else:\n",
        "        return 'netral'\n",
        "\n",
        "df['label'] = df['score'].apply(score_to_label)\n",
        "# Hitung jumlah tiap label\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v8pTwik-uej",
        "outputId": "87b00fe6-2d90-4794-a1a2-f8fd9db981d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "negatif    5734\n",
            "positif    4266\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt6ZaZYT_Aqu",
        "outputId": "db637b8f-194a-4a12-bc84-d532e08d0531"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   userName  10000 non-null  object\n",
            " 1   content   10000 non-null  object\n",
            " 2   score     10000 non-null  int64 \n",
            " 3   at        10000 non-null  object\n",
            " 4   label     10000 non-null  object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 390.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "OiRqzRof_H-u",
        "outputId": "6c7b5c19-c63c-4957-cf2b-c7442ea9ac71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "userName    0\n",
              "content     0\n",
              "score       0\n",
              "at          0\n",
              "label       0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>userName</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>content</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>score</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>at</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI2tpGkm_Ndl",
        "outputId": "a2e61214-e29a-4c32-b6ba-a9391d0da770"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Undersampling\n",
        "target = 4000  # Sesuaikan dengan jumlah data minoritas\n",
        "df_under = (\n",
        "    df.groupby('label', group_keys=False)\n",
        "    .apply(lambda x: x.head(min(target, len(x))))\n",
        ")\n",
        "print(f\"\\nJumlah data setelah undersampling: {len(df_under)}\")\n",
        "print(\"\\nDistribusi label setelah undersampling:\")\n",
        "print(df_under['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtmyiZoj_Odc",
        "outputId": "fd7dedfa-36ef-4abb-c803-43af3400f0a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jumlah data setelah undersampling: 8000\n",
            "\n",
            "Distribusi label setelah undersampling:\n",
            "label\n",
            "negatif    4000\n",
            "positif    4000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi preprocessing\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emotikon\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # simbol & pictograph\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # bendera\n",
        "        u\"\\U00002700-\\U000027BF\"  # dingbats\n",
        "        u\"\\U0001F900-\\U0001F9FF\"  # simbol tambahan\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"  # simbol tambahan 2\n",
        "        u\"\\U00002600-\\U000026FF\"  # simbol misc\n",
        "        u\"\\U00002000-\\U000023FF\"  # simbol tambahan\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    return emoji_pattern.sub('', text)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()  # case folding\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # hapus URL\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)  # hapus mention & hashtag\n",
        "    text = re.sub(r'\\d+', '', text)  # hapus angka\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # hapus tanda baca\n",
        "    text = remove_emoji(text)  # hapus emoji\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = clean_text(text)\n",
        "    return text\n",
        "\n",
        "# Terapkan cleaning\n",
        "print(\"\\nMelakukan text cleaning...\")\n",
        "df_under['content'] = df_under['content'].apply(preprocess_text)\n",
        "df_under['content'] = df_under['content'].apply(lambda x: x.split())\n",
        "print(df_under.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObLkulSu_ywT",
        "outputId": "e75e38d2-5b33-4b46-d67f-925714568bfb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Melakukan text cleaning...\n",
            "                userName                                            content  \\\n",
            "0   UCIL OLOL LEHO [UOL]  [kenapa, indosat, ada, tuyulnya, sekarang, pul...   \n",
            "2              Xeraphine  [tolong, itu, sistem, login, nomor, utama, dan...   \n",
            "9              mang pecu  [pelayanan, bayar, bulanan, iseng, isi, pulsa,...   \n",
            "10         Adelia Raidha  [im, jaringannyaa, jelek, banget, percumaa, mo...   \n",
            "11         Hayato Hayato  [sinyal, jelek, rugi, make, m, sudah, paket, m...   \n",
            "\n",
            "    score                   at    label  \n",
            "0       1  2024-02-02 05:17:07  negatif  \n",
            "2       3  2024-02-02 05:13:29  negatif  \n",
            "9       1  2024-02-02 04:45:17  negatif  \n",
            "10      1  2024-02-02 04:40:51  negatif  \n",
            "11      1  2024-02-02 04:39:42  negatif  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Slang word normalization\n",
        "try:\n",
        "    def read_dictionary_from_file(file_path):\n",
        "        with open(file_path, 'r') as file:\n",
        "            dictionary = json.load(file)\n",
        "        return dictionary\n",
        "\n",
        "    file_path = 'slangwords.txt'\n",
        "    my_dictionary = read_dictionary_from_file(file_path)\n",
        "\n",
        "    def change(tokens):\n",
        "        new_list = []\n",
        "        for token in tokens:\n",
        "            if token in my_dictionary:\n",
        "                new_list.append(my_dictionary[token])\n",
        "            else:\n",
        "                new_list.append(token)\n",
        "        return new_list\n",
        "\n",
        "    print(\"Melakukan normalisasi slang words...\")\n",
        "    df_under['content'] = df_under['content'].apply(change)\n",
        "except FileNotFoundError:\n",
        "    print(\"File slangwords.txt tidak ditemukan, skip normalisasi slang words\")\n",
        "print(df_under.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzhu6Zed_8t2",
        "outputId": "175ec748-d73b-4507-9904-5a9ee918dd2e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melakukan normalisasi slang words...\n",
            "                userName                                            content  \\\n",
            "0   UCIL OLOL LEHO [UOL]  [kenapa, indosat, ada, tuyulnya, sekarang, pul...   \n",
            "2              Xeraphine  [tolong, itu, sistem, login, nomor, utama, dan...   \n",
            "9              mang pecu  [pelayanan, bayar, bulanan, iseng, isi, pulsa,...   \n",
            "10         Adelia Raidha  [im, jaringannyaa, jelek, banget, percumaa, mo...   \n",
            "11         Hayato Hayato  [sinyal, jelek, rugi, make, m, sudah, paket, m...   \n",
            "\n",
            "    score                   at    label  \n",
            "0       1  2024-02-02 05:17:07  negatif  \n",
            "2       3  2024-02-02 05:13:29  negatif  \n",
            "9       1  2024-02-02 04:45:17  negatif  \n",
            "10      1  2024-02-02 04:40:51  negatif  \n",
            "11      1  2024-02-02 04:39:42  negatif  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopword removal\n",
        "try:\n",
        "    def import_words_from_file(file_path):\n",
        "        word_list = []\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                word_list.append(line.strip())\n",
        "        return word_list\n",
        "\n",
        "    file_path = 'stopwords.txt'\n",
        "    stopwords = import_words_from_file(file_path)\n",
        "\n",
        "    def stopword_removal(tokens):\n",
        "        new_list = []\n",
        "        for token in tokens:\n",
        "            if token not in stopwords:\n",
        "                new_list.append(token)\n",
        "        return new_list\n",
        "\n",
        "    print(\"Melakukan stopword removal...\")\n",
        "    df_under['content'] = df_under['content'].apply(stopword_removal)\n",
        "except FileNotFoundError:\n",
        "    print(\"File stopwords.txt tidak ditemukan, skip stopword removal\")\n",
        "print(df_under.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyxZACawAJVP",
        "outputId": "d1d53398-2b80-4dab-aa1b-82f652679e8d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melakukan stopword removal...\n",
            "                userName                                            content  \\\n",
            "0   UCIL OLOL LEHO [UOL]  [indosat, tuyulnya, pulsa, berkurang, tidak, p...   \n",
            "2              Xeraphine  [sistem, login, nomor, utama, nomor, sekunder,...   \n",
            "9              mang pecu  [pelayanan, bayar, bulanan, iseng, isi, pulsa,...   \n",
            "10         Adelia Raidha  [im, jaringannyaa, jelek, banget, percumaa, mo...   \n",
            "11         Hayato Hayato  [sinyal, jelek, rugi, make, m, paket, mahal, s...   \n",
            "\n",
            "    score                   at    label  \n",
            "0       1  2024-02-02 05:17:07  negatif  \n",
            "2       3  2024-02-02 05:13:29  negatif  \n",
            "9       1  2024-02-02 04:45:17  negatif  \n",
            "10      1  2024-02-02 04:40:51  negatif  \n",
            "11      1  2024-02-02 04:39:42  negatif  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "print(\"Melakukan stemming...\")\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stemming(tokens):\n",
        "    new_list = []\n",
        "    for token in tokens:\n",
        "        if token:  # Skip empty tokens\n",
        "            stemmed = stemmer.stem(token)\n",
        "            new_list.append(stemmed)\n",
        "    return new_list\n",
        "\n",
        "df_under['content'] = df_under['content'].apply(stemming)\n",
        "print(df_under.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjAaAstdAdBz",
        "outputId": "7ebb93b5-da68-4851-8b38-6ab547e11233"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melakukan stemming...\n",
            "                userName                                            content  \\\n",
            "0   UCIL OLOL LEHO [UOL]  [indosat, tuyul, pulsa, kurang, tidak, paket, ...   \n",
            "2              Xeraphine  [sistem, login, nomor, utama, nomor, sekunder,...   \n",
            "9              mang pecu  [layan, bayar, bulan, iseng, isi, pulsa, perlu...   \n",
            "10         Adelia Raidha  [im, jaringannyaa, jelek, banget, percumaa, mo...   \n",
            "11         Hayato Hayato  [sinyal, jelek, rugi, make, m, paket, mahal, s...   \n",
            "\n",
            "    score                   at    label  \n",
            "0       1  2024-02-02 05:17:07  negatif  \n",
            "2       3  2024-02-02 05:13:29  negatif  \n",
            "9       1  2024-02-02 04:45:17  negatif  \n",
            "10      1  2024-02-02 04:40:51  negatif  \n",
            "11      1  2024-02-02 04:39:42  negatif  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hapus data dengan content kosong setelah preprocessing\n",
        "df_under = df_under[df_under['content'].apply(len) > 0].copy()\n",
        "print(f\"\\nJumlah data setelah hapus content kosong: {len(df_under)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWyOy2xsAlPp",
        "outputId": "c0b55b63-7251-417e-debf-d449d04c92fd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jumlah data setelah hapus content kosong: 7498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Label encoding\n",
        "label_map = {'negatif': 0, 'positif': 1}\n",
        "df_under['label_num'] = df_under['label'].map(label_map)\n",
        "\n",
        "print(\"\\nContoh data setelah preprocessing:\")\n",
        "print(df_under[['content', 'score', 'label', 'label_num']].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6i2XUUqAp5g",
        "outputId": "cb64758c-84de-4f49-e032-1cc6b0fd027e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contoh data setelah preprocessing:\n",
            "                                              content  score    label  \\\n",
            "0   [indosat, tuyul, pulsa, kurang, tidak, paket, ...      1  negatif   \n",
            "2   [sistem, login, nomor, utama, nomor, sekunder,...      3  negatif   \n",
            "9   [layan, bayar, bulan, iseng, isi, pulsa, perlu...      1  negatif   \n",
            "10  [im, jaringannyaa, jelek, banget, percumaa, mo...      1  negatif   \n",
            "11  [sinyal, jelek, rugi, make, m, paket, mahal, s...      1  negatif   \n",
            "12                    [tidak, registrasi, isi, pulsa]      1  negatif   \n",
            "20                      [beli, internet, tidak, guna]      2  negatif   \n",
            "22                             [top, up, game, tidak]      1  negatif   \n",
            "24  [kecewa, banget, kartu, beli, paket, gb, habis...      1  negatif   \n",
            "31  [aplikasi, g, benar, beli, kuota, padah, data,...      1  negatif   \n",
            "\n",
            "    label_num  \n",
            "0           0  \n",
            "2           0  \n",
            "9           0  \n",
            "10          0  \n",
            "11          0  \n",
            "12          0  \n",
            "20          0  \n",
            "22          0  \n",
            "24          0  \n",
            "31          0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPLITTING DATA"
      ],
      "metadata": {
        "id": "rZlJ7nreBGQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_texts = df_under['content'].values  # Array of token lists\n",
        "y = df_under['label_num'].values\n",
        "\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    X_texts, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "Jc94SSumBFs1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nData training: {len(X_train_texts)} samples\")\n",
        "print(f\"Data testing: {len(X_test_texts)} samples\")\n",
        "print(f\"\\nDistribusi label training:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "for label, count in zip(unique, counts):\n",
        "    label_name = 'Negatif' if label == 0 else 'Positif'\n",
        "    print(f\"  {label_name} ({label}): {count}\")\n",
        "print(f\"\\nDistribusi label testing:\")\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "for label, count in zip(unique, counts):\n",
        "    label_name = 'Negatif' if label == 0 else 'Positif'\n",
        "    print(f\"  {label_name} ({label}): {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07aAHZBTB4Uf",
        "outputId": "f492fc75-8409-4f72-8b66-89d47b4e0b54"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data training: 5998 samples\n",
            "Data testing: 1500 samples\n",
            "\n",
            "Distribusi label training:\n",
            "  Negatif (0): 3153\n",
            "  Positif (1): 2845\n",
            "\n",
            "Distribusi label testing:\n",
            "  Negatif (0): 788\n",
            "  Positif (1): 712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Embedding"
      ],
      "metadata": {
        "id": "8K9V3FgbAs84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train = X_train_texts.tolist()\n",
        "print(f\"\\nJumlah kalimat untuk training Word2Vec: {len(sentences_train)}\")\n",
        "\n",
        "# Parameter Word2Vec\n",
        "vector_size = 100  # dimensi vektor\n",
        "window = 5  # jendela konteks\n",
        "min_count = 2  # minimum frekuensi kata\n",
        "workers = 4  # jumlah thread\n",
        "sg = 1  # 1 = skip-gram, 0 = CBOW\n",
        "\n",
        "print(f\"\\nParameter Word2Vec:\")\n",
        "print(f\"- Vector size: {vector_size}\")\n",
        "print(f\"- Window: {window}\")\n",
        "print(f\"- Min count: {min_count}\")\n",
        "print(f\"- Algorithm: {'Skip-gram' if sg == 1 else 'CBOW'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MO9w7ykAvH8",
        "outputId": "afad56a1-e7fe-4381-abd3-d10efe274910"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Jumlah kalimat untuk training Word2Vec: 5998\n",
            "\n",
            "Parameter Word2Vec:\n",
            "- Vector size: 100\n",
            "- Window: 5\n",
            "- Min count: 2\n",
            "- Algorithm: Skip-gram\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(\n",
        "    sentences=sentences_train,\n",
        "    vector_size=vector_size,\n",
        "    window=window,\n",
        "    min_count=min_count,\n",
        "    workers=workers,\n",
        "    sg=sg,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "print(f\"\\nVocabulary size: {len(w2v_model.wv)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXcq0dSYA6Mf",
        "outputId": "7e6ecf57-b6be-4b82-d4ce-d6377660602d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary size: 1595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mengubah kalimat menjadi vektor rata-rata\n",
        "def sentence_to_vec(sentence, model):\n",
        "    vectors = []\n",
        "    for word in sentence:\n",
        "        if word in model.wv:\n",
        "            vectors.append(model.wv[word])\n",
        "\n",
        "    if len(vectors) > 0:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)"
      ],
      "metadata": {
        "id": "tkSoo0AGA6_0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konversi data training ke vektor\n",
        "print(\"\\nMengkonversi data training ke vektor...\")\n",
        "X_train = np.array([sentence_to_vec(sent, w2v_model) for sent in sentences_train])\n",
        "print(f\"Shape X_train: {X_train.shape}\")\n",
        "\n",
        "# Konversi data testing ke vektor\n",
        "print(\"\\nMengkonversi data testing ke vektor...\")\n",
        "sentences_test = X_test_texts.tolist()\n",
        "X_test = np.array([sentence_to_vec(sent, w2v_model) for sent in sentences_test])\n",
        "print(f\"Shape X_test: {X_test.shape}\")\n",
        "\n",
        "print(f\"\\nShape label y_train: {y_train.shape}\")\n",
        "print(f\"Shape label y_test: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3v2GkErCNCA",
        "outputId": "954da2d0-f8a9-4510-91a0-a98d1ac6b96d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mengkonversi data training ke vektor...\n",
            "Shape X_train: (5998, 100)\n",
            "\n",
            "Mengkonversi data testing ke vektor...\n",
            "Shape X_test: (1500, 100)\n",
            "\n",
            "Shape label y_train: (5998,)\n",
            "Shape label y_test: (1500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM TRAINING"
      ],
      "metadata": {
        "id": "Z04NnrOdCV7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi SVM dengan kernel RBF\n",
        "svm_model = SVC(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    gamma='scale',\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\nParameter SVM:\")\n",
        "print(f\"- Kernel: {svm_model.kernel}\")\n",
        "print(f\"- C: {svm_model.C}\")\n",
        "print(f\"- Gamma: {svm_model.gamma}\")\n",
        "\n",
        "print(\"\\nMulai training SVM...\")\n",
        "svm_model.fit(X_train, y_train)\n",
        "print(\"Training selesai!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTn1J0VuCVHI",
        "outputId": "fdacbf74-58bb-4b79-f695-f50746df7701"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Parameter SVM:\n",
            "- Kernel: rbf\n",
            "- C: 1.0\n",
            "- Gamma: scale\n",
            "\n",
            "Mulai training SVM...\n",
            "[LibSVM]Training selesai!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediksi\n",
        "y_pred_train = svm_model.predict(X_train)\n",
        "y_pred_test = svm_model.predict(X_test)\n",
        "\n",
        "# Akurasi\n",
        "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"\\nAkurasi Training: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
        "print(f\"Akurasi Testing: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqGCbhSpCQhS",
        "outputId": "37c9de5a-cded-4b96-cb82-cffc20afec7f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Akurasi Training: 0.8631 (86.31%)\n",
            "Akurasi Testing: 0.8687 (86.87%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"CLASSIFICATION REPORT (TEST SET)\")\n",
        "print(\"-\"*50)\n",
        "target_names = ['Negatif', 'Positif']\n",
        "print(classification_report(y_test, y_pred_test, target_names=target_names))\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"\\n\" + \"-\"*50)\n",
        "print(\"CONFUSION MATRIX (TEST SET)\")\n",
        "print(\"-\"*50)\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "print(\"\\n           Predicted\")\n",
        "print(\"           Negatif  Positif\")\n",
        "print(f\"Actual Negatif  {cm[0][0]:6d}  {cm[0][1]:6d}\")\n",
        "print(f\"       Positif  {cm[1][0]:6d}  {cm[1][1]:6d}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVpKl7Q-CdNA",
        "outputId": "dddaf196-9580-491e-debb-3df73ab3cfc0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--------------------------------------------------\n",
            "CLASSIFICATION REPORT (TEST SET)\n",
            "--------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Negatif       0.85      0.92      0.88       788\n",
            "     Positif       0.90      0.82      0.86       712\n",
            "\n",
            "    accuracy                           0.87      1500\n",
            "   macro avg       0.87      0.87      0.87      1500\n",
            "weighted avg       0.87      0.87      0.87      1500\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "CONFUSION MATRIX (TEST SET)\n",
            "--------------------------------------------------\n",
            "\n",
            "           Predicted\n",
            "           Negatif  Positif\n",
            "Actual Negatif     722      66\n",
            "       Positif     131     581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Model"
      ],
      "metadata": {
        "id": "bcKO25EYCgaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(text, w2v_model, svm_model, stemmer):\n",
        "    # Preprocessing\n",
        "    processed = preprocess_text(text)\n",
        "    tokens = processed.split()\n",
        "\n",
        "    # Try slang normalization\n",
        "    try:\n",
        "        tokens = change(tokens)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Try stopword removal\n",
        "    try:\n",
        "        tokens = stopword_removal(tokens)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Stemming\n",
        "    tokens = stemming(tokens)\n",
        "\n",
        "    # Convert to vector\n",
        "    vec = sentence_to_vec(tokens, w2v_model)\n",
        "    vec = vec.reshape(1, -1)\n",
        "\n",
        "    # Predict\n",
        "    prediction = svm_model.predict(vec)[0]\n",
        "    sentiment = 'Positif' if prediction == 1 else 'Negatif'\n",
        "\n",
        "    return sentiment, tokens\n",
        "\n",
        "# Contoh testing berdasarkan data asli\n",
        "test_samples = [\n",
        "    \"Kenapa Indosat ada tuyulnya sekarang pulsa tiba-tiba berkurang\",\n",
        "    \"aplikasi ini sangat bagus\",\n",
        "    \"Pelayanan bayar bulanan buruk\",\n",
        "]\n",
        "\n",
        "for i, sample in enumerate(test_samples, 1):\n",
        "    sentiment, tokens = predict_sentiment(sample, w2v_model, svm_model, stemmer)\n",
        "    print(f\"\\n{i}. Text: {sample}\")\n",
        "    print(f\"   Tokens: {tokens}\")\n",
        "    print(f\"   Prediksi Sentimen: {sentiment}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PROSES SELESAI\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdHIH_LsCjJF",
        "outputId": "59eda7a5-cd84-494a-ab94-a6122e25ac6c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Text: Kenapa Indosat ada tuyulnya sekarang pulsa tiba-tiba berkurang\n",
            "   Tokens: ['indosat', 'tuyul', 'pulsa', 'kurang']\n",
            "   Prediksi Sentimen: Negatif\n",
            "\n",
            "2. Text: aplikasi ini sangat bagus\n",
            "   Tokens: ['aplikasi', 'bagus']\n",
            "   Prediksi Sentimen: Positif\n",
            "\n",
            "3. Text: Pelayanan bayar bulanan buruk\n",
            "   Tokens: ['layan', 'bayar', 'bulan', 'buruk']\n",
            "   Prediksi Sentimen: Negatif\n",
            "\n",
            "==================================================\n",
            "PROSES SELESAI\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model\n",
        "print(\"\\nMenyimpan model...\")\n",
        "w2v_model.save(\"word2vec_model.model\")\n",
        "import pickle\n",
        "with open('svm_model.pkl', 'wb') as f:\n",
        "    pickle.dump(svm_model, f)\n",
        "\n",
        "# Simpan juga data preprocessed untuk dashboard\n",
        "df_under.to_csv('data_preprocessed.csv', index=False)\n",
        "print(\"Model dan data berhasil disimpan!\")\n",
        "print(\"\\nFile yang dihasilkan:\")\n",
        "print(\"- word2vec_model.model\")\n",
        "print(\"- svm_model.pkl\")\n",
        "print(\"- data_preprocessed.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXcfYj5JCtGK",
        "outputId": "1b068635-a216-418d-fd71-b6c10242b0fb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Menyimpan model...\n",
            "Model dan data berhasil disimpan!\n",
            "\n",
            "File yang dihasilkan:\n",
            "- word2vec_model.model\n",
            "- svm_model.pkl\n",
            "- data_preprocessed.csv\n"
          ]
        }
      ]
    }
  ]
}